## [正则化综述](https://zhuanlan.zhihu.com/p/67931198)

机器学习中经常会在损失函数中加入正则项，称之为正则化（Regularize）。

**目的**：==防止模型过拟合==

**原理**：==在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性==

**例子**：以最简单的线性模型为例

$$
y=X\beta + \epsilon
$$

我们在统计学习中接触到线性回归的**最小二乘估计和正则化的岭回归与拉索回归**。

$$
\hat{\beta}=arg \min_{\beta}||y-X\beta||^2
$$
在最小二乘估计中==加入​正则项==后，我们得到==岭估计==：

$$
\tilde{\beta}=arg\min_{\beta}||y-X\beta||^2+\lambda||\beta||^2
$$

在数学上我们可以证明==*岭估计*的参数模==要**严格小于**==*最小二乘估计*的参数模==，换句话说，**我们可以认为加入==L2正则项==后，估计参数长度变短了，这在数学上被称为特征缩减**（shrinkage）。

> **shrinkage方法介绍**：指训练求解参数过程中考虑到系数的大小，通过设置惩罚系数，使得影响较小的特征的系数衰减到0，只保留重要特征的从而减少模型复杂度进而达到规避过拟合的目的。常用的shinkage的方法有==Lasso（L1正则化）==和==岭回归（L2正则化）==等。  
> **采用shrinkage方法的==主要目的包括两个==**：*一方面* 因为模型可能考虑到很多==没必要的特征==，这些特征对于模型来说就==是噪声==，shrinkage可以通过==消除噪声==从而减少模型复杂度；*另一方面* 模型==特征存在多重共线性==（变量之间相互关联）的话可能导致==模型多解==，而多解模型的一个解往往不能反映模型的真实情况，shrinkage可以==消除关联的特征==**提高模型稳定性**。

## 通过线性模型理解正则化

对于包括多元线性回归模型、逻辑回归和SVM在内的线性模型，我们需要利用==测试集来最小化**损失函数**==从而求得模型参数w。

$$
E(w)=\frac{1}{2}\sum_{i=1}^{N}\{y_i-w^T\phi(x_i)\}^2
$$

**我们在==线性模型的损失函数==中==加入正则项==可以得到目标函数**。其中λ被称为==正则化系数==，当**λ越大**时，正则化**约束越强**。

$$
\frac{1}{2}\sum_{i=1}^{N}\{y_i-w^T\phi(x_i)\}^2+\frac{\lambda}{2}w^Tw
$$

​​​​​​​通过令目标函数导函数为0，我们可以得到参数的表达式为：

$$
w=(\lambda I+\Phi^T\Phi)^{-1}\Phi^TY
$$

## ==选择L2正则项==的原因

给损失函数加上的正则化项可以有多种形式，下面给出了正则化的一般形式：

$$
\frac{1}{2} \sum_{i=1}^{N}{y_i-w^T\phi(x_i)}^2+\frac{\lambda}{2}\sum_{i=1}{M} |w_j|^q
$$

其中M是==参数的个数==，也是模型==特征的维数==； q 是正则项的==阶数==，L2正则项的 q 为2。

考虑到在高维数据下很难给出正则项的几何意义，我们假设<font color="#ff0000">数据源只有<span style="background:#40a9ff"><font color="#ff0000">两个特征</font></span></font>：

$$
x = \{x_1, x_2\}, w= \{w_1,w_2\}
$$

​​​​​​​ q **不同取值时正则项的函数值图像：**

  

![](https://pic4.zhimg.com/80/v2-db118d5b3c20dc0be9a7037ad4299783_1440w.webp)

**不同函数值图像对应的等高线（即俯视图）为**：

  

![](https://pic1.zhimg.com/80/v2-a2e87c039e606aef88c383e803fda12c_1440w.webp)

图像等高线

==最小化目标函数时==，可以看做在控制损失函数不变的情况时==令正则项最小化==，几何意义如下所示：蓝色圈表示==没有限制的损失函数随着 w 迭代寻找着最小化的过程的 E(w) 函数等高线==（同个圆上的损失函数值相同），**蓝色圈和橙色圈之和就是目标函数值，目标函数最小化的点往往出现在*==蓝圈==和==橙圈==相交的点*==即目标函数最小化的参数值==** w∗ 。

<span style="background:#40a9ff"><font color="#ffffff">蓝圈和中心指输出和真实值的误差？</font></span>

![](https://pic1.zhimg.com/80/v2-ab2c59de73c373f6d852cd1a4577ed38_1440w.webp)

目标函数最小化的几何展示

可以看到，==L1正则化的最优==参数值 w∗ ==恰好是 w1=0 的时候==，意味着我们剔除了模型中一个特征（系数为0等价于剔除该特征），**从而达到了降低模型复杂度的目的**。在这个意义上L1正则化效果要优于L2正则化，**但L1存在拐点不是处处可微，从而==L2正则化有更好的求解特性==**。

## 总结

梳理一下，正则化有多种方式，包括L0（向量中非零元素个数），L1（向量中元素绝对值之和），L2（向量的模）。**但是L0范数的求解是个NP完全问题，而L1也能实现稀疏并且比L0有更好的优化求解特性而被广泛应用**。

L2范数指各元素平方和后开根的值，可令 w 每个元素接近于0，**虽然不如L1更彻底地降低模型复杂度，但是由于处处可微降低了计算难度**

发布于 2019-06-03 18:27