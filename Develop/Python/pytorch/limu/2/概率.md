/chapter_preliminaries/probability.ipynb

## 基本概率论

假设我们掷骰子，想知道看到1的几率有多大，而不是看到另一个数字。
如果骰子是公平的，那么所有六个结果$\{1, \ldots, 6\}$都有相同的可能发生，
因此我们可以说$1$发生的概率为$\frac{1}{6}$。

然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。
检查骰子的唯一方法是多次投掷并记录结果。
==对于每个骰子，我们将观察到$\{1, \ldots, 6\}$中的一个值。==
==对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，==
即此*事件*（event）概率的*估计值*。
*大数定律*（law of large numbers）告诉我们：
随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。

在统计学中，我们把从概率分布中抽取样本的过程称为*抽样*（sampling）。
笼统来说，可以把*分布*（distribution）看作对事件的概率分配，
稍后我们将给出的更正式定义。
将概率分配给一些离散选择的分布称为*多项分布*（multinomial distribution）。

为了==抽取一个样本==，即掷骰子，我们只需==传入一个**概率向量**==。
==输出是另一个**相同长度**的向量==：它在索引$i$处的值是采样结果中$i$==出现的次数==。
```python
from torch.distributions import multinomial
fair_probs = torch.ones([6]) / 6
sample = multinomial.Multinomial(1, fair_probs).sample()
# tensor([0., 0., 1., 0., 0., 0.])
```

在估计一个骰子的公平性时，我们希望从同一分布中==生成多个样本==。 如果用Python的for循环来完成这个任务，速度会慢得惊人。 因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。
```python
multinomial.Multinomial(10, fair_probs).sample()
# tensor([5., 3., 2., 0., 0., 0.])

# 将结果存储为32位浮点数以进行除法
counts = multinomial.Multinomial(1000, fair_probs).sample() # shape 1，6 一组包括1000次概率
counts / 1000  # 相对频率作为估计值
# tensor([0.1550, 0.1820, 0.1770, 0.1710, 0.1600, 0.1550])
```

我们也可以看到这些概率如何随着时间的推移收敛到真实概率。 让我们进行500组实验，==每组抽取10个样本==。
```python
counts = multinomial.Multinomial(10, fair_probs).sample((500,)) # shape 500,6，每组10次概率和
cum_counts = counts.cumsum(dim=0) # 返回维度dim中输入元素的累计和(dim=0即每列按所有行累加)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True) # 某列次数 / 当前行的各列样本总数
''' counts:
tensor([[0., 2., 1., 4., 1., 2.],
        [0., 1., 1., 2., 3., 3.],
        [1., 0., 1., 2., 4., 2.],
        ...,
        [0., 1., 1., 3., 1., 4.],
        [0., 6., 2., 1., 0., 1.],
        [3., 1., 3., 0., 0., 3.]])
        
cum_counts:
tensor([[  0.,   2.,   1.,   4.,   1.,   2.],
        [  0.,   3.,   2.,   6.,   4.,   5.],
        [  1.,   3.,   3.,   8.,   8.,   7.],
        ...,
        [852., 817., 798., 836., 820., 857.],
        [852., 823., 800., 837., 820., 858.],
        [855., 824., 803., 837., 820., 861.]])
        
estimates:
tensor([[0.2000, 0.1000, 0.2000, 0.2000, 0.1000, 0.2000],
        [0.1500, 0.1000, 0.2500, 0.2000, 0.1000, 0.2000],
        [0.1667, 0.1333, 0.1667, 0.2000, 0.1333, 0.2000],
        ...,
        [0.1721, 0.1735, 0.1610, 0.1663, 0.1681, 0.1590],
        [0.1717, 0.1735, 0.1613, 0.1659, 0.1683, 0.1591],
        [0.1720, 0.1734, 0.1612, 0.1660, 0.1686, 0.1588]])
'''
from d2l import torch as d2l
d2l.set_figsize((6, 4.5)) # 6类样本
for i in range(6): # 选中各列所有行，拼label
    d2l.plt.plot(estimates[:, i].numpy(), label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed') # y轴参考线
d2l.plt.gca().set_xlabel('Groups of experiments') # x,y轴
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend()
print("break here...") # 命令行启动此处需要断点 or
# d2l.plt.pause(100)
d2l.plt.show()
# help(torch.cumsum)
'''
cumsum(...)
    cumsum(input, dim, *, dtype=None, out=None) -> Tensor
    Returns the cumulative(累积的) sum of elements of :attr:`input` in the dimension
'''
```