/chapter_preliminaries/probability.ipynb

## 基本概率论

假设我们掷骰子，想知道看到1的几率有多大，而不是看到另一个数字。
如果骰子是公平的，那么所有六个结果$\{1, \ldots, 6\}$都有相同的可能发生，
因此我们可以说$1$发生的概率为$\frac{1}{6}$。

然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。
检查骰子的唯一方法是多次投掷并记录结果。
==对于每个骰子，我们将观察到$\{1, \ldots, 6\}$中的一个值。==
==对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，==
即此*事件*（event）概率的*估计值*。
*大数定律*（law of large numbers）告诉我们：
随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。

在统计学中，我们把从概率分布中抽取样本的过程称为*抽样*（sampling）。
笼统来说，可以把*分布*（distribution）看作对事件的概率分配，
稍后我们将给出的更正式定义。
将概率分配给一些离散选择的分布称为*多项分布*（multinomial distribution）。

为了==抽取一个样本==，即掷骰子，我们只需==传入一个**概率向量**==。
==输出是另一个**相同长度**的向量==：它在索引$i$处的值是采样结果中$i$==出现的次数==。
```python
from torch.distributions import multinomial
fair_probs = torch.ones([6]) / 6
sample = multinomial.Multinomial(1, fair_probs).sample()
```

在估计一个骰子的公平性时，我们希望从同一分布中==生成多个样本==。 如果用Python的for循环来完成这个任务，速度会慢得惊人。 因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。
```python
multinomial.Multinomial(10, fair_probs).sample()
# 将结果存储为32位浮点数以进行除法
counts = multinomial.Multinomial(1000, fair_probs).sample()
counts / 1000  # 相对频率作为估计值
```

```
tensor([5., 3., 2., 0., 0., 0.])
tensor([0.1550, 0.1820, 0.1770, 0.1710, 0.1600, 0.1550])
```