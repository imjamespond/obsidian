### 3.1. 线性回归[¶](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#sec-linear-regression "Permalink to this heading")
![[Pasted image 20231018143450.png|800]]
[(3.1.1)](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#equation-eq-price-area)中的$w_{\mathrm{area}}$和$w_{\mathrm{age}}$
称为*权重*（weight），权重决定了每个特征对我们预测值的影响。
==$b$称为*偏置*（bias）==、*偏移量*（offset）或*截距*（intercept）。
==**偏置是指当所有特征都取值为0时，预测值应该为多少。**==
即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。
如果没有偏置项，我们模型的表达能力将受到限制。
严格来说， :eqref:`eq_price-area`是输入特征的一个
*仿射变换*（affine transformation）。
仿射变换的特点是通过加权和对特征进行*线性变换*（linear transformation），
并通过偏置项来进行*平移*（translation）。


而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。
当我们的输入包含$d$个特征时，我们将预测结果$\hat{y}$
（通常使用“尖角”符号表示$y$的估计值）表示为：
$$\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$


将所有==特征放到向量𝐱==∈ℝ𝑑�∈��中， 并将所有==权重放到向量𝐰==∈ℝ𝑑�∈��中， 我们可以用==点积==形式来简洁地表达模型：
𝑦̂ =𝐰⊤𝐱+𝑏.


`eq_linreg-y`中， ==向量𝐱�对应于**单个数据样本**的特征==。 用符号表示的==矩阵𝐗==∈ℝ𝑛×𝑑�∈��×� 可以很方便地引用我们==整个数据集的𝑛�个样本==。 其中，==𝐗�的每一**行**是一个**样本**，每一**列**是一种**特征**。==
对于特征集合𝐗�，预测值𝐲̂ ∈ℝ𝑛�^∈�� 可以通过==矩阵-向量乘法==表示为：
𝐲̂ =𝐗𝐰+𝑏


<font color="#ff0000" size=5 >梯度下降</font>最简单的==用法是计算损失函数==（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常<font color="#ff0000">慢</font>：因为在每一次更新参数之前，我们必须==遍历整个数据集==。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做_==小批量随机梯度下降==_（minibatch stochastic gradient descent）。


在每次迭代中，我们首先**随机抽样一个小批量B**�， 它是由固定数量的训练**样本**组成的。 然后，我们计算==小批量的平均损失关于模型参数的导数（也可以称为梯度）==。 最后，我们将梯度乘以一个预先确定的正数𝜂�，并从当前参数的值中减掉。

我们用下面的数学公式来表示这一更新过程（∂∂表示偏导数）：
